"ray":
  "init":
    "local_mode": false
    "log_to_driver": true
    "logging_level": "WARNING"
#    "reuse_actors": true
  "run_experiments":
    "experiments":
      "impala_rnn_global_agent":
        "run": "IMPALA"
        "checkpoint_freq": 1
        "checkpoint_at_end": true
        "stop":
          "training_iteration": 20
        "config":
          "log_level": "WARNING"

          ####################
          ####################
          # IMPALA
          ####################
          ####################

          # V-trace params (see vtrace.py).
          "vtrace": True
          "vtrace_clip_rho_threshold": 1.0
          "vtrace_clip_pg_rho_threshold": 1.0
          # System params.
          #
          # == Overview of data flow in IMPALA ==
          # 1. Policy evaluation in parallel across `num_workers` actors produces
          #    batches of size `rollout_fragment_length * num_envs_per_worker`.
          # 2. If enabled, the replay buffer stores and produces batches of size
          #    `rollout_fragment_length * num_envs_per_worker`.
          # 3. If enabled, the minibatch ring buffer stores and replays batches of
          #    size `train_batch_size` up to `num_sgd_iter` times per batch.
          # 4. The learner thread executes data parallel SGD across `num_gpus` GPUs
          #    on batches of size `train_batch_size`.
          #
          "rollout_fragment_length": 50
          "train_batch_size": 500
          "min_iter_time_s": 10
          "num_workers": 7
          # number of GPUs the learner should use.
          "num_gpus": 1
          # set >1 to load data into GPUs in parallel. Increases GPU memory usage
          # proportionally with the number of buffers.
          "num_data_loader_buffers": 1
          # how many train batches should be retained for minibatching. This conf
          # only has an effect if `num_sgd_iter > 1`.
          "minibatch_buffer_size": 1
          # number of passes to make over each train batch
          "num_sgd_iter": 1
          # set >0 to enable experience replay. Saved samples will be replayed with
          # a p:1 proportion to new data samples.
          "replay_proportion": 0.0
          # number of sample batches to store for replay. The number of transitions
          # saved total will be (replay_buffer_num_slots * rollout_fragment_length).
          "replay_buffer_num_slots": 0
          # max queue size for train batches feeding into the learner
          "learner_queue_size": 16
          # wait for train batches to be available in minibatch buffer queue
          # this many seconds. This may need to be increased e.g. when training
          # with a slow environment
          "learner_queue_timeout": 300
          # level of queuing for sampling.
          "max_sample_requests_in_flight_per_worker": 2
          # max number of workers to broadcast one set of weights to
          "broadcast_interval": 1
          # use intermediate actors for multi-level aggregation. This can make sense
          # if ingesting >2GB/s of samples, or if the data requires decompression.
          "num_aggregation_workers": 0

          # Learning params.
          "grad_clip": 40.0
          # either "adam" or "rmsprop"
          "opt_type": "adam"
          "lr": 0.0005
          "lr_schedule": null
          # rmsprop considerd
          "decay": 0.99
          "momentum": 0.0
          "epsilon": 0.1
          # balancing the three losses
          "vf_loss_coeff": 0.5
          "entropy_coeff": 0.01
          "entropy_coeff_schedule": null

          # use fake (infinite speed) sampler for testing
          "_fake_sampler": False

          ####################
          ####################
          # EVALUATION
          ####################
          ####################

          "evaluation_interval": 1 # Evaluate with every `evaluation_interval` training iterations.
          "evaluation_num_episodes": 10
          "in_evaluation": False
          "evaluation_config": {
             "explore": False
          }
          "evaluation_num_workers": 0
          "custom_eval_function": null
          "use_exec_api": False

          ####################
          ####################
          # EXPLORATION
          ####################
          ####################

          "exploration_config":
            "type": "EpsilonGreedy"
            "epsilon_schedule":
                "type": "ExponentialSchedule"
                "schedule_timesteps": 200000
                "initial_p": 1.0
                "decay_rate": 0.001

          ####################
          ####################
          # MODEL
          ####################
          ####################

          "model":
#            "custom_model": "tf_raw_obs_rnn_model"
#            "custom_options":
#              "lstm_cell_size": 128
#              "max_seq_len": 999999
            "custom_model": "my_keras_rnn"

          ####################
          ####################
          # ENV
          ####################
          ####################
          "gamma": 0.995
          "horizon": null # if null, horizon will be choosen by env
          "env": "green_wave"
          "env_config":
            "render": false
            "simulator": "traci"
            "sim_params":
              "restart_instance": True
              "sim_step": 1
              "print_warnings": False
              "render": False
            "env_state_params": null
            "agents_params":
              "name": "OneAgentForAllIntersections"
              "params":
                "default_policy": null
                "action_params":
                  "name": "MultiNodesPhaseConnector"
                  "params": {}
                "obs_params":
                  "name": "MultiNodesRawConnector"
                  "params":
                    "flat_observation": true
                    "detector_position": [5, 100]
                    "phase_channel": true
                "reward_params":
                  "name": "MultiNodesQueueRewardConnector"
                  "params":
                    "stop_speed": 2
"general":
  "id": "main"
  "seed": null
  "repeat": 1
  "is_tensorboardX": false
  "sumo_home": "/home/ncarrara/sumo_binaries/bin"
  "workspace": "tmp/main"
  "logging":
    "version": 1
    "disable_existing_loggers": false
    "formatters":
      "standard":
        "format": "[%(name)s] %(levelname)s - %(message)s"
    "handlers":
      "default":
        "level": "WARNING"
        "formatter": "standard"
        "class": "logging.StreamHandler"
    "loggers":
      "":
        "handlers": ["default"]
        "level": "WARNING"
        "propagate": false
      "some.logger.you.want.to.enable.in.the.code":
        "handlers": ["default"]
        "level": "ERROR"
        "propagate": false
        
      
    
  


