"ray":
  "init":
    "local_mode": false
    "log_to_driver": true
    "logging_level": "WARNING"
#    "reuse_actors": true
  "run_experiments":
    "experiments":
      "a3c_tdtse_global_agent":
        "run": "A3C"
        "checkpoint_freq": 20
        "checkpoint_at_end": true
#        "stop":
#          "training_iteration": 20
        "config":
          "log_level": "WARNING"
          "num_workers": 7
          "num_gpus": 1
          ####################
          ####################
          # A3C
          ####################
          ####################

          # Should use a critic as a baseline (otherwise don't use value baseline;
          # required for using GAE).
          "use_critic": True
          # If true, use the Generalized Advantage Estimator (GAE)
          # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.
          "use_gae": True
          # Size of rollout batch
          "rollout_fragment_length": 500
          # GAE(gamma) parameter
          "lambda": 1.0
          # Max global norm for each gradient calculated by worker
          "grad_clip": 40.0
          # Learning rate
          "lr": 0.0001
          # Learning rate schedule
          "lr_schedule": None
          # Value Function Loss coefficient
          "vf_loss_coeff": 0.5
          # Entropy coefficient
          "entropy_coeff": 0.01
          # Min time per iteration
          "min_iter_time_s": 5
          # Workers sample async. Note that this increases the effective
          # rollout_fragment_length by up to 5x due to async buffering of batches.
          "sample_async": True
          # Use the execution plan API instead of policy optimizers.
          "use_exec_api": True
#          ####################
#          ####################
#          # EVALUATION
#          ####################
#          ####################
#
#          "evaluation_interval": 1 # Evaluate with every `evaluation_interval` training iterations.
#          "evaluation_num_episodes": 10
#          "in_evaluation": False
#          "evaluation_config": {
#             "explore": False
#          }
#          "evaluation_num_workers": 0
#          "custom_eval_function": null
#          "use_exec_api": False

          ####################
          ####################
          # EXPLORATION
          ####################
          ####################

#          "exploration_config":
#            "type": "EpsilonGreedy"
#            "epsilon_schedule":
#                "type": "ExponentialSchedule"
#                "schedule_timesteps": 200000
#                "initial_p": 1.0
#                "decay_rate": 0.001

          ####################
          ####################
          # MODEL
          ####################
          ####################

          "model":
            "custom_model": "multi_nodes_tdtse"
            "custom_options":
              "filters_size": 32
              "dense_layer_size": 32
              "use_progression": false
#            "custom_model": "tf_raw_obs_rnn_model"
#            "custom_options":
#              "lstm_cell_size": 128
#              "max_seq_len": 999999
#            "custom_model": "my_keras_rnn"

          ####################
          ####################
          # ENV
          ####################
          ####################
          "gamma": 0.995
          "horizon": null # if null, horizon will be choosen by env
          "env": "green_wave"
          "env_config":
            "render": false
            "simulator": "traci"
            "sim_params":
              "restart_instance": True
              "sim_step": 1
              "print_warnings": False
              "render": False
            "env_state_params": null
            "agents_params":
              "name": "OneAgentForAllIntersections"
              "params":
                "default_policy": null
                "action_params":
                  "name": "MultiNodesPhaseConnector"
                  "params": {}
                "obs_params":
                  "name": "MultiNodesTDTSEConnector"
                  "params":
                    "obs_params":
                      "num_history": 60
                      "detector_position": [5, 100]
                    "phase_channel": true
                "reward_params":
                  "name": "MultiNodesQueueRewardConnector"
                  "params":
                    "stop_speed": 2
"general":
  "id": "main"
  "seed": null
  "repeat": 1
  "is_tensorboardX": false
  "sumo_home": "/home/ncarrara/sumo_binaries/bin"
  "workspace": "tmp/main"
  "logging":
    "version": 1
    "disable_existing_loggers": false
    "formatters":
      "standard":
        "format": "[%(name)s] %(levelname)s - %(message)s"
    "handlers":
      "default":
        "level": "WARNING"
        "formatter": "standard"
        "class": "logging.StreamHandler"
    "loggers":
      "":
        "handlers": ["default"]
        "level": "WARNING"
        "propagate": false
      "some.logger.you.want.to.enable.in.the.code":
        "handlers": ["default"]
        "level": "ERROR"
        "propagate": false
        
      
    
  


