"ray":
  "init":
    "local_mode": false
    "log_to_driver": true
    "logging_level": "WARNING"
#    "reuse_actors": true
  "run_experiments":
    "experiments":
      "ppo_rnn_global_agent":
        "run": "PPO"
        "checkpoint_freq": 1
        "checkpoint_at_end": true
#        "stop":
#          "training_iteration": 20
        "config":
          "log_level": "WARNING"
          "num_workers": 7
          "num_gpus": 1
          ####################
          ####################
          # PPO
          ####################
          ####################

#          # Should use a critic as a baseline (otherwise don't use value baseline;
#          # required for using GAE).
#          "use_critic": True
#          # If true, use the Generalized Advantage Estimator (GAE)
#          # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.
#          "use_gae": True
#          # The GAE(lambda) parameter.
#          "lambda": 1.0
#          # Initial coefficient for KL divergence.
#          "kl_coeff": 0.2
#          # Size of batches collected from each worker.
#          "rollout_fragment_length": 200
#          # Number of timesteps collected for each SGD round. This defines the size
#          # of each SGD epoch.
#          "train_batch_size": 4000
#          # Total SGD batch size across all devices for SGD. This defines the
#          # minibatch size within each epoch.
#          "sgd_minibatch_size": 128
#          # Whether to shuffle sequences in the batch when training (recommended).
#          "shuffle_sequences": True
#          # Number of SGD iterations in each outer loop (i.e., number of epochs to
#          # execute per train batch).
#          "num_sgd_iter": 30
#          # Stepsize of SGD.
#          "lr": 5e-5
#          # Learning rate schedule.
#          "lr_schedule": None
#          # Share layers for value function. If you set this to True, it's important
#          # to tune vf_loss_coeff.
#          "vf_share_layers": False
#          # Coefficient of the value function loss. IMPORTANT: you must tune this if
#          # you set vf_share_layers: True.
#          "vf_loss_coeff": 1.0
#          # Coefficient of the entropy regularizer.
#          "entropy_coeff": 0.0
#          # Decay schedule for the entropy regularizer.
#          "entropy_coeff_schedule": None
#          # PPO clip parameter.
#          "clip_param": 0.3
#          # Clip param for the value function. Note that this is sensitive to the
#          # scale of the rewards. If your expected V is large, increase this.
#          "vf_clip_param": 10.0
#          # If specified, clip the global norm of gradients by this amount.
#          "grad_clip": None
#          # Target value for KL divergence.
#          "kl_target": 0.01
#          # Whether to rollout "complete_episodes" or "truncate_episodes".
#          "batch_mode": "truncate_episodes"
#          # Which observation filter to apply to the observation.
#          "observation_filter": "NoFilter"
#          # Uses the sync samples optimizer instead of the multi-gpu one. This is
#          # usually slower, but you might want to try it if you run into issues with
#          # the default optimizer.
#          "simple_optimizer": False
#          # Use PyTorch as framework?
#          "use_pytorch": False
#
#          ####################
#          ####################
#          # EVALUATION
#          ####################
#          ####################
#
#          "evaluation_interval": 1 # Evaluate with every `evaluation_interval` training iterations.
#          "evaluation_num_episodes": 10
#          "in_evaluation": False
#          "evaluation_config": {
#             "explore": False
#          }
#          "evaluation_num_workers": 0
#          "custom_eval_function": null
#          "use_exec_api": False

          ####################
          ####################
          # EXPLORATION
          ####################
          ####################

#          "exploration_config":
#            "type": "EpsilonGreedy"
#            "epsilon_schedule":
#                "type": "ExponentialSchedule"
#                "schedule_timesteps": 200000
#                "initial_p": 1.0
#                "decay_rate": 0.001

          ####################
          ####################
          # MODEL
          ####################
          ####################

          "model":
#            "custom_model": "tf_raw_obs_rnn_model"
#            "custom_options":
#              "lstm_cell_size": 128
#              "max_seq_len": 999999
            "custom_model": "my_keras_rnn"

          ####################
          ####################
          # ENV
          ####################
          ####################
          "gamma": 0.995
          "horizon": null # if null, horizon will be choosen by env
          "env": "green_wave"
          "env_config":
            "render": false
            "simulator": "traci"
            "sim_params":
              "restart_instance": True
              "sim_step": 1
              "print_warnings": False
              "render": False
            "env_state_params": null
            "agents_params":
              "name": "OneAgentForAllIntersections"
              "params":
                "default_policy": null
                "action_params":
                  "name": "MultiNodesPhaseConnector"
                  "params": {}
                "obs_params":
                  "name": "MultiNodesRawConnector"
                  "params":
                    "flat_observation": true
                    "detector_position": [5, 100]
                    "phase_channel": true
                "reward_params":
                  "name": "MultiNodesQueueRewardConnector"
                  "params":
                    "stop_speed": 2
"general":
  "id": "main"
  "seed": null
  "repeat": 1
  "is_tensorboardX": false
  "sumo_home": "/home/ncarrara/sumo_binaries/bin"
  "workspace": "tmp/main"
  "logging":
    "version": 1
    "disable_existing_loggers": false
    "formatters":
      "standard":
        "format": "[%(name)s] %(levelname)s - %(message)s"
    "handlers":
      "default":
        "level": "WARNING"
        "formatter": "standard"
        "class": "logging.StreamHandler"
    "loggers":
      "":
        "handlers": ["default"]
        "level": "WARNING"
        "propagate": false
      "some.logger.you.want.to.enable.in.the.code":
        "handlers": ["default"]
        "level": "ERROR"
        "propagate": false
        
      
    
  


